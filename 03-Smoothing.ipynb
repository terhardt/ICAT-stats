{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of smoothing on correlations\n",
    "\n",
    "To illustrate the effect of smoothing on the null-distribution of correlation coefficients we can run a small simulation study:\n",
    "\n",
    "We generate a number of white noise samples that we correlate to generate a baseline null-distribution that emulates what a classical t-test would test against. We than filter the same time series with increasing $sigma$ and recalculate the correlations to obtain null-distributions for the smoothed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 2000   # number of observations\n",
    "n_rep = 10000  # number of repetitions\n",
    "\n",
    "# Generate n_rep white noise time series with length n_obs\n",
    "x = np.random.randn(n_rep, n_obs)\n",
    "y = np.random.randn(n_rep, n_obs)\n",
    "\n",
    "# Calculate correlation between pairs of unfiltered time series\n",
    "# This will serve as our baseline\n",
    "r_null_unfilted = np.array([pearsonr(xi, yi)[0] for xi, yi in zip(x, y)])\n",
    "\n",
    "# Filter width we want to look at\n",
    "sigma_filt = np.array((2, 5, 10, 20))\n",
    "# Output array\n",
    "r_null = np.zeros((len(sigma_filt), n_rep))\n",
    "\n",
    "# Filter white noise with the different filter lengths and save correlation coefficients for later\n",
    "for i, s in enumerate(sigma_filt):\n",
    "    xf = gaussian_filter1d(np.random.randn(n_rep, n_obs), s)\n",
    "    yf = gaussian_filter1d(np.random.randn(n_rep, n_obs), s)\n",
    "    r_null[i] = np.array([pearsonr(xi, yi)[0] for xi, yi in zip(xf, yf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets take a look at histograms for the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-0.5, 0.5, 50)\n",
    "plt.hist(r_null_unfilted, bins, histtype='step', density=True, color='k', label='unfilted')\n",
    "for i, s in enumerate(sigma_filt):\n",
    "    plt.hist(r_null[i], bins, histtype='step', density=True, label='$\\sigma=%.1f$' % s)\n",
    "    \n",
    "plt.xlabel('$r$')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend()\n",
    "plt.title('Null-distribution for $r$, gaussian smoothing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the empirical distributions to calculate the value for $r$ that is the 95th percentile in the unsmoothed data. This value needs to be exeeded for the correlation to be significant at the (1 - 0.95) = 0.05 significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_95 = np.percentile(r_null_unfilted, 95)\n",
    "print('Empircial r_crit for unsmoothed data: %.3f', r_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again using the samples we just generated, we can check the fraction of correlations that exeeds this threshhold. This gives an indication of how often we would call a random correlation significant, if we were to use the threshold of the white noise hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = np.mean(np.abs(r_null) >= r_95, axis=1)\n",
    "print('P(r>=r_95) for smoothed data:', fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the rapid increase in the false-positive rate we can plot these values against the filter widths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigma_filt, np.mean(np.abs(r_null) > r_95, axis=1), 'k.-')\n",
    "plt.xlabel('Width of gaussian filter ($\\sigma$)')\n",
    "plt.ylabel('False positive rate at $\\\\alpha=%.2f$' % (0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "To go back to the example from the lecture we load the data and plot it alongside the two smoothed versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('smoothing_example_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, sharex=True, figsize=(12, 5))\n",
    "\n",
    "sigma_f = 10\n",
    "\n",
    "for i, (ax, c) in enumerate(zip(axes, d.columns)):\n",
    "    ax.plot(d.index, d[c], '0.3', lw=0.75, label='annual')\n",
    "    ax.plot(d.index, gaussian_filter1d(d[c], sigma_f), 'r', lw=2.0, label='%u yr gaussian' % sigma_f)\n",
    "    ax.set_ylabel('Anomaly (Â°C)')\n",
    "    ax.set_xlim(0, 2000)\n",
    "    ax.text(0.01, 0.95, 'Site %u' %(i+1), va='top', ha='left', transform=ax.transAxes, color='k', fontsize=12)\n",
    "    \n",
    "axes[1].legend(loc='upper center', ncol=2)\n",
    "axes[-1].set_xlabel('Year (C.E.)')\n",
    "\n",
    "\n",
    "print('Annual: r=%.3f, (p=%.6f)' % pearsonr(d['y1'], d['y2']))\n",
    "print('%u yr : r=%.3f, (p=%.6f)' % (sigma_f, *pearsonr(gaussian_filter1d(d['y1'], sigma_f), \n",
    "                                                 gaussian_filter1d(d['y2'], sigma_f))))\n",
    "\n",
    "# Save correlation value for later\n",
    "corr = pearsonr(gaussian_filter1d(d['y1'], sigma_f), \n",
    "                gaussian_filter1d(d['y2'], sigma_f))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a more relistic null-distribution we use again AR(1) processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ar1 import fit_ar1, sample_ar1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we estimate the auto-correlation as well as the standard deviations of the AR(1) processes from the data.\n",
    "\n",
    "We see that both time series have very high auto-correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_y1 = fit_ar1(d.iloc[:,0].values - d.iloc[:,0].values.mean())\n",
    "ar_y2 = fit_ar1(d.iloc[:,1].values - d.iloc[:,0].values.mean())\n",
    "\n",
    "print('y1 (phi, sigma_e):', ar_y1)\n",
    "print('y2 (phi, sigma_e):', ar_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We than generate a large number of samples from AR(1) processes with these parametres that have the same number of observations as the original data. These we correlate with each other to generate the null-distribution we compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = d.shape[0]\n",
    "n_samples = 10000\n",
    "\n",
    "y1_samples = gaussian_filter1d(sample_ar1(n_obs, *ar_y1, n_samples), sigma_f)\n",
    "y2_samples = gaussian_filter1d(sample_ar1(n_obs, *ar_y2, n_samples), sigma_f)\n",
    "r_null_dist = np.array([pearsonr(y1i, y2i)[0] for y1i, y2i in zip(y1_samples, y2_samples)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the the null distribution together with the correlation of the smoothed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(r_null_dist, density=True, bins=25, range=(-0.8, 0.8), histtype='step', color='k')\n",
    "ax.axvline(corr, lw=2.0, ls='dashed')\n",
    "ax.set_xlabel('$r$')\n",
    "ax.set_ylabel('Probability density')\n",
    "ax.set_title('Empirical null distribution (smoothed AR(1))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets calculate the empirical p-value of this correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = np.mean(np.abs(r_null_dist) >= corr)\n",
    "print('r=%.2f, p=%.2f' % (corr, p_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of smoothing on correlations\n",
    "\n",
    "To illustrate the effect of smoothing on the null-distribution of correlation coefficients we can run a small simulation study:\n",
    "\n",
    "We generate a number of white noise samples that we correlate to generate a baseline null-distribution that emulates what a classical t-test would test against. We than filter the same time series with increasing $\\sigma$ and recalculate the correlations to obtain null-distributions for the smoothed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 2000   # number of observations\n",
    "n_rep = 10000  # number of repetitions\n",
    "\n",
    "# Generate n_rep white noise time series with length n_obs\n",
    "x = np.random.randn(n_rep, n_obs)\n",
    "y = np.random.randn(n_rep, n_obs)\n",
    "\n",
    "# Calculate correlation between pairs of unfiltered time series\n",
    "# This will serve as our baseline\n",
    "r_null_unfilted = np.array([pearsonr(xi, yi)[0] for xi, yi in zip(x, y)])\n",
    "\n",
    "# Filter width we want to look at\n",
    "sigma_filt = np.array((2, 5, 10, 20))\n",
    "# Output array\n",
    "r_null = np.zeros((len(sigma_filt), n_rep))\n",
    "\n",
    "# Filter white noise with the different filter lengths and save correlation coefficients for later\n",
    "for i, s in enumerate(sigma_filt):\n",
    "    xf = gaussian_filter1d(np.random.randn(n_rep, n_obs), s)\n",
    "    yf = gaussian_filter1d(np.random.randn(n_rep, n_obs), s)\n",
    "    r_null[i] = np.array([pearsonr(xi, yi)[0] for xi, yi in zip(xf, yf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets take a look at histograms for the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-0.5, 0.5, 50)\n",
    "plt.hist(r_null_unfilted, bins, histtype='step', density=True, color='k', label='unfilted')\n",
    "for i, s in enumerate(sigma_filt):\n",
    "    plt.hist(r_null[i], bins, histtype='step', density=True, label='$\\sigma=%.1f$' % s)\n",
    "    \n",
    "plt.xlabel('$r$')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend()\n",
    "plt.title('Null-distribution for $r$, gaussian smoothing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the empirical distributions to calculate the value for $r$ that is the 95th percentile in the unsmoothed data. This value needs to be exeeded for the correlation to be significant at the (1 - 0.95) = 0.05 significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_95 = np.percentile(r_null_unfilted, 95)\n",
    "print('Empircial r_crit for unsmoothed data: %.3f', r_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the samples, we just generated, we can check the fraction of correlations that exeeds this threshhold. This gives an indication of how often we would call a random correlation significant, if we were to use the threshold of the white noise hypothesis. We call this the false-positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = np.mean(np.abs(r_null) >= r_95, axis=1)\n",
    "print('P(r>=r_95) for smoothed data:', fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the rapid increase in the false-positive rate we can plot these values against the filter widths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigma_filt, np.mean(np.abs(r_null) > r_95, axis=1), 'k.-')\n",
    "plt.xlabel('Width of gaussian filter ($\\sigma$)')\n",
    "plt.ylabel('False positive rate at $\\\\alpha=%.2f$' % (0.05))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

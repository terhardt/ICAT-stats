{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "To go back to the example from the lecture we load the data and plot it alongside the two smoothed versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('smoothing_example_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, sharex=True, figsize=(12, 5))\n",
    "\n",
    "sigma_f = 10\n",
    "\n",
    "for i, (ax, c) in enumerate(zip(axes, d.columns)):\n",
    "    ax.plot(d.index, d[c], '0.3', lw=0.75, label='annual')\n",
    "    ax.plot(d.index, gaussian_filter1d(d[c], sigma_f), 'r', lw=2.0, label='%u yr gaussian' % sigma_f)\n",
    "    ax.set_ylabel('Anomaly (Â°C)')\n",
    "    ax.set_xlim(0, 2000)\n",
    "    ax.text(0.01, 0.95, 'Site %u' %(i+1), va='top', ha='left', transform=ax.transAxes, color='k', fontsize=12)\n",
    "    \n",
    "axes[1].legend(loc='upper center', ncol=2)\n",
    "axes[-1].set_xlabel('Year (C.E.)')\n",
    "\n",
    "\n",
    "print('Annual: r=%.3f, (p=%.6f)' % pearsonr(d['y1'], d['y2']))\n",
    "print('%u yr : r=%.3f, (p=%.6f)' % (sigma_f, *pearsonr(gaussian_filter1d(d['y1'], sigma_f), \n",
    "                                                 gaussian_filter1d(d['y2'], sigma_f))))\n",
    "\n",
    "# Save correlation value for later\n",
    "corr = pearsonr(gaussian_filter1d(d['y1'], sigma_f), \n",
    "                gaussian_filter1d(d['y2'], sigma_f))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a more relistic null-distribution we use again AR(1) processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ar1 import fit_ar1, sample_ar1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we estimate the auto-correlation as well as the standard deviations of the AR(1) processes from the data.\n",
    "\n",
    "We see that both time series have very high auto-correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_y1 = fit_ar1(d.iloc[:,0].values - d.iloc[:,0].values.mean())\n",
    "ar_y2 = fit_ar1(d.iloc[:,1].values - d.iloc[:,0].values.mean())\n",
    "\n",
    "print('y1 (phi, sigma_e):', ar_y1)\n",
    "print('y2 (phi, sigma_e):', ar_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We than generate a large number of samples from AR(1) processes with these parametres that have the same number of observations as the original data. These we correlate with each other to generate the null-distribution we compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = d.shape[0]\n",
    "n_samples = 10000\n",
    "\n",
    "y1_samples = gaussian_filter1d(sample_ar1(n_obs, *ar_y1, n_samples), sigma_f)\n",
    "y2_samples = gaussian_filter1d(sample_ar1(n_obs, *ar_y2, n_samples), sigma_f)\n",
    "r_null_dist = np.array([pearsonr(y1i, y2i)[0] for y1i, y2i in zip(y1_samples, y2_samples)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the the null distribution together with the correlation of the smoothed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(r_null_dist, density=True, bins=25, range=(-0.8, 0.8), histtype='step', color='k')\n",
    "ax.axvline(corr, lw=2.0, ls='dashed')\n",
    "ax.set_xlabel('$r$')\n",
    "ax.set_ylabel('Probability density')\n",
    "ax.set_title('Empirical null distribution (smoothed AR(1))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets calculate the empirical p-value of this correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = np.mean(np.abs(r_null_dist) >= corr)\n",
    "print('r=%.2f, p=%.2f' % (corr, p_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
